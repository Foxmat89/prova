{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-21T10:05:57.962420Z","iopub.execute_input":"2022-04-21T10:05:57.963125Z","iopub.status.idle":"2022-04-21T10:05:57.968341Z","shell.execute_reply.started":"2022-04-21T10:05:57.963093Z","shell.execute_reply":"2022-04-21T10:05:57.967368Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## 1. Installation","metadata":{}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:06:06.163079Z","iopub.execute_input":"2022-04-21T10:06:06.163582Z","iopub.status.idle":"2022-04-21T10:06:56.389762Z","shell.execute_reply.started":"2022-04-21T10:06:06.163539Z","shell.execute_reply":"2022-04-21T10:06:56.388725Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## 2. Spark Session","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:06:56.392191Z","iopub.execute_input":"2022-04-21T10:06:56.393111Z","iopub.status.idle":"2022-04-21T10:07:01.625976Z","shell.execute_reply.started":"2022-04-21T10:06:56.393072Z","shell.execute_reply":"2022-04-21T10:07:01.623657Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## 3. Dummy data creation","metadata":{}},{"cell_type":"code","source":"sampleData = [\n    {\n        \"employee\": \"James\", \"department\": \"Sales\", \"state\": \"NY\", \"salary\": 90000, \"age\": 34, \"bonus\": 10000\n    },\n    {\n        \"employee\": \"Michael\", \"department\": \"Sales\", \"state\": \"NY\", \"salary\": 86000, \"age\": 56, \"bonus\": 20000\n    },\n    {\n        \"employee\": \"Robert\", \"department\": \"Sales\", \"state\": \"CA\", \"salary\": 81000, \"age\": 39, \"bonus\": 23000\n    },\n    {\n        \"employee\": \"Maria\", \"department\": \"Finance\", \"state\": \"CA\", \"salary\": 90000, \"age\": 24, \"bonus\": 23000\n    },\n    {\n        \"employee\": \"Raman\", \"department\": \"Finance\", \"state\": \"CA\", \"salary\": 99000, \"age\": 40, \"bonus\": 24000\n    },\n    {\n        \"employee\": \"Scott\", \"department\": \"Finance\", \"state\": \"NY\", \"salary\": 83000, \"age\": 36, \"bonus\": 19000\n    },\n    {\n        \"employee\": \"Jen\", \"department\": \"Finance\", \"state\": \"NY\", \"salary\": 79000, \"age\": 53, \"bonus\": 15000\n    },\n    {\n        \"employee\": \"Jeff\", \"department\": \"Marketing\", \"state\": \"CA\", \"salary\": 80000, \"age\": 25, \"bonus\": 18000\n    },\n    {\n        \"employee\": \"Kumar\", \"department\": \"Marketing\", \"state\": \"NY\", \"salary\": 91000, \"age\": 50, \"bonus\": 21000\n    }\n]\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:08.814098Z","iopub.execute_input":"2022-04-21T10:07:08.814392Z","iopub.status.idle":"2022-04-21T10:07:08.825167Z","shell.execute_reply.started":"2022-04-21T10:07:08.814361Z","shell.execute_reply":"2022-04-21T10:07:08.824086Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## 4. create Spark DataFrame and show","metadata":{}},{"cell_type":"code","source":"# to read from a .csv file\n# df = spark.read.csv(\"<file_path>\")\n\ndf = spark.createDataFrame(sampleData)\ndf.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:13.242168Z","iopub.execute_input":"2022-04-21T10:07:13.242475Z","iopub.status.idle":"2022-04-21T10:07:19.462472Z","shell.execute_reply.started":"2022-04-21T10:07:13.242436Z","shell.execute_reply":"2022-04-21T10:07:19.461641Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### show as a pandas df","metadata":{}},{"cell_type":"code","source":"df.limit(5).toPandas()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:23.702229Z","iopub.execute_input":"2022-04-21T10:07:23.702517Z","iopub.status.idle":"2022-04-21T10:07:24.057290Z","shell.execute_reply.started":"2022-04-21T10:07:23.702487Z","shell.execute_reply":"2022-04-21T10:07:24.056279Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### print schema","metadata":{}},{"cell_type":"code","source":"df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:26.247145Z","iopub.execute_input":"2022-04-21T10:07:26.247858Z","iopub.status.idle":"2022-04-21T10:07:26.254458Z","shell.execute_reply.started":"2022-04-21T10:07:26.247822Z","shell.execute_reply":"2022-04-21T10:07:26.253862Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### rename columns","metadata":{}},{"cell_type":"code","source":"df = df.withColumnRenamed(\"employee\", \"employee_name\")\ndf.printSchema()\n\n## rename for all columns\n# df = df.toDF(*[\"age_xy\", \"bonus_xy\", \"department_xy\", \"employee_xy\", \"salary_xy\", \"state_xy\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:27.527144Z","iopub.execute_input":"2022-04-21T10:07:27.527442Z","iopub.status.idle":"2022-04-21T10:07:27.547726Z","shell.execute_reply.started":"2022-04-21T10:07:27.527407Z","shell.execute_reply":"2022-04-21T10:07:27.546872Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## select columns","metadata":{}},{"cell_type":"code","source":"df1 = df.select(\"employee_name\", \"age\", \"state\")\ndf1.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:28.812280Z","iopub.execute_input":"2022-04-21T10:07:28.812801Z","iopub.status.idle":"2022-04-21T10:07:29.185171Z","shell.execute_reply.started":"2022-04-21T10:07:28.812752Z","shell.execute_reply":"2022-04-21T10:07:29.184282Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## sort","metadata":{}},{"cell_type":"code","source":"## by default, in ascending order\ndf.sort('age').show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:29.942172Z","iopub.execute_input":"2022-04-21T10:07:29.942869Z","iopub.status.idle":"2022-04-21T10:07:30.398634Z","shell.execute_reply.started":"2022-04-21T10:07:29.942832Z","shell.execute_reply":"2022-04-21T10:07:30.397998Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"## sort in descending order\nfrom pyspark.sql import functions as F\n\ndf.sort(F.desc('age')).show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:30.474020Z","iopub.execute_input":"2022-04-21T10:07:30.474377Z","iopub.status.idle":"2022-04-21T10:07:30.737856Z","shell.execute_reply.started":"2022-04-21T10:07:30.474331Z","shell.execute_reply":"2022-04-21T10:07:30.736982Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## create a new column using Spark UDF","metadata":{}},{"cell_type":"code","source":"# normal python function\ndef salary_in_k(x):\n    return x/1000\n\n# convert the above function to spark UDF and return type of function (Typecast)\nfrom pyspark.sql import types as T\n\nsalary_in_k_udf = F.udf(salary_in_k, T.DoubleType())\n\n# creatinfg column\ndf = df.withColumn('salary_in_k', salary_in_k_udf(F.col('salary')))\n\ndf.show()\n                   ","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:32.007218Z","iopub.execute_input":"2022-04-21T10:07:32.007516Z","iopub.status.idle":"2022-04-21T10:07:33.209051Z","shell.execute_reply.started":"2022-04-21T10:07:32.007484Z","shell.execute_reply":"2022-04-21T10:07:33.208094Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Filter","metadata":{}},{"cell_type":"code","source":"df.filter((df.age >= 50) & (df.salary_in_k >= 80.0)).show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:33.241911Z","iopub.execute_input":"2022-04-21T10:07:33.242168Z","iopub.status.idle":"2022-04-21T10:07:33.943287Z","shell.execute_reply.started":"2022-04-21T10:07:33.242136Z","shell.execute_reply":"2022-04-21T10:07:33.942381Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## groupBy","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:47:36.577725Z","iopub.execute_input":"2022-04-21T09:47:36.578062Z","iopub.status.idle":"2022-04-21T09:47:36.581688Z","shell.execute_reply.started":"2022-04-21T09:47:36.578028Z","shell.execute_reply":"2022-04-21T09:47:36.581003Z"}}},{"cell_type":"code","source":"# groupBy on single column with sum agg\ndf.groupBy('department').sum('salary').show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:34.662242Z","iopub.execute_input":"2022-04-21T10:07:34.663205Z","iopub.status.idle":"2022-04-21T10:07:35.659926Z","shell.execute_reply.started":"2022-04-21T10:07:34.663149Z","shell.execute_reply":"2022-04-21T10:07:35.659063Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# groupBy on multiple columns\ndf.groupBy('department', 'state').sum('salary', 'bonus').show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:35.661443Z","iopub.execute_input":"2022-04-21T10:07:35.661775Z","iopub.status.idle":"2022-04-21T10:07:36.199868Z","shell.execute_reply.started":"2022-04-21T10:07:35.661733Z","shell.execute_reply":"2022-04-21T10:07:36.199018Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# running more aggregations at a time\ndf.groupBy('department').agg(F.sum('salary'), F.avg('salary'),\n                            F.min('bonus'), F.max('bonus')).show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:36.201380Z","iopub.execute_input":"2022-04-21T10:07:36.201664Z","iopub.status.idle":"2022-04-21T10:07:36.668153Z","shell.execute_reply.started":"2022-04-21T10:07:36.201623Z","shell.execute_reply":"2022-04-21T10:07:36.666932Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# using filter and alias on the aggreated data\ndf.groupBy('department').agg(F.sum('salary').alias('sum_salary'), \n                             F.avg('salary').alias('avg_salary'),\n                             F.min('bonus').alias('min_bonus'), \n                             F.max('bonus').alias('max_bonus')).where(\n                                                                F.col('min_bonus') >= 15000).show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:43.868239Z","iopub.execute_input":"2022-04-21T10:07:43.868578Z","iopub.status.idle":"2022-04-21T10:07:44.300390Z","shell.execute_reply.started":"2022-04-21T10:07:43.868544Z","shell.execute_reply":"2022-04-21T10:07:44.299160Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"## collect aggregated list\ndf.groupBy('department').agg(F.collect_list('state')).alias('state_list').show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:44.397463Z","iopub.execute_input":"2022-04-21T10:07:44.398058Z","iopub.status.idle":"2022-04-21T10:07:44.755475Z","shell.execute_reply.started":"2022-04-21T10:07:44.398020Z","shell.execute_reply":"2022-04-21T10:07:44.754614Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Joins","metadata":{}},{"cell_type":"code","source":"df1 = df.select(\"employee_name\", \"age\", \"state\", \"salary_in_k\")\ndf2 = df.select(\"salary\", \"department\", \"bonus\", \"employee_name\", \"age\")\ndf1.show(), df2.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:07:57.042657Z","iopub.execute_input":"2022-04-21T10:07:57.042956Z","iopub.status.idle":"2022-04-21T10:07:57.563718Z","shell.execute_reply.started":"2022-04-21T10:07:57.042924Z","shell.execute_reply":"2022-04-21T10:07:57.562962Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"df = df1.join(df2, [\"employee_name\", \"age\"], how='inner') # how='left'/'right'\ndf.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:08:02.742343Z","iopub.execute_input":"2022-04-21T10:08:02.742591Z","iopub.status.idle":"2022-04-21T10:08:03.515132Z","shell.execute_reply.started":"2022-04-21T10:08:02.742566Z","shell.execute_reply":"2022-04-21T10:08:03.514270Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# when one table is too big and other is too small -> try broadcasting/map side joins\ndf = df1.join(F.broadcast(df2), [\"employee_name\", \"age\"], how='inner')\ndf.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:10:40.093280Z","iopub.execute_input":"2022-04-21T10:10:40.094248Z","iopub.status.idle":"2022-04-21T10:10:40.586731Z","shell.execute_reply.started":"2022-04-21T10:10:40.094190Z","shell.execute_reply":"2022-04-21T10:10:40.585822Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Use SQL with DataFrame","metadata":{}},{"cell_type":"code","source":"# register the df to a table\ndf.registerTempTable('df_table')\n\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nsqlContext.sql('select * from df_table where age >= 50').show()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:17:56.656627Z","iopub.execute_input":"2022-04-21T10:17:56.657654Z","iopub.status.idle":"2022-04-21T10:17:57.253755Z","shell.execute_reply.started":"2022-04-21T10:17:56.657602Z","shell.execute_reply":"2022-04-21T10:17:57.252872Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Using RDD","metadata":{}},{"cell_type":"code","source":"import math\nfrom pyspark.sql import Row\n\ndef row_function(row):\n    # convert row to python dict()\n    row_dict = row.asDict()\n    \n    # add a new key and value\n    row_dict['exp_bonus'] = float(np.log(row_dict['bonus']))\n    \n    # convert dict back to row \n    newRow = Row(**row_dict)\n    \n    return newRow\n\n# convert df to RDD\ndf_rdd = df.rdd\n\n# apply above function to RDD\ndf_rdd_new = df_rdd.map(lambda row: row_function(row))\n\n# convert rdd back to DataFrame\ndf_new = spark.createDataFrame(df_rdd_new)\n\ndf_new.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:28:32.703683Z","iopub.execute_input":"2022-04-21T10:28:32.704089Z","iopub.status.idle":"2022-04-21T10:28:33.005388Z","shell.execute_reply.started":"2022-04-21T10:28:32.704055Z","shell.execute_reply":"2022-04-21T10:28:33.004458Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Using Pandas UDF","metadata":{}},{"cell_type":"code","source":"df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:30:23.755229Z","iopub.execute_input":"2022-04-21T10:30:23.755572Z","iopub.status.idle":"2022-04-21T10:30:23.761419Z","shell.execute_reply.started":"2022-04-21T10:30:23.755534Z","shell.execute_reply":"2022-04-21T10:30:23.760602Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"## get normalized bonus grouped by department\n\n# Declare the schema for the output of our function\noutSchema = T.StructType([T.StructField('employee_name', T.StringType(),True),\n                        T.StructField('age', T.LongType(),True),\n                        T.StructField('state', T.StringType(),True),\n                        T.StructField('salary', T.LongType(),True),\n                        T.StructField('salary_in_k', T.DoubleType(),True),\n                        T.StructField('department', T.StringType(),True),\n                        T.StructField('bonus', T.LongType(),True),\n                        T.StructField('normalized_bonus', T.DoubleType(),True)\n                       ])\n\n# decorate our function with pandas_udf decorator\n@F.pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP)\ndef subtract_mean(pdf):\n    # pdf is a pandas.DataFrame\n    v = pdf.bonus\n    v = v - v.mean()\n    pdf['normalized_bonus'] = v\n    \n    return pdf\n\nconfirmed_groupwise_normalization = df.groupby(\"department\").apply(subtract_mean)\n\nconfirmed_groupwise_normalization.toPandas()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:37:03.003638Z","iopub.execute_input":"2022-04-21T10:37:03.003938Z","iopub.status.idle":"2022-04-21T10:37:03.953380Z","shell.execute_reply.started":"2022-04-21T10:37:03.003899Z","shell.execute_reply":"2022-04-21T10:37:03.952313Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Mllib","metadata":{}},{"cell_type":"code","source":"from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer\n\n# category encoding\nindexers = [StringIndexer(inputCol=column, outputCol=column+'_idx'\n                         ) for column in [\"department\", \"employee_name\", \"state\"]]\n\n# create pieline for indexers\npipeline = Pipeline(stages=indexers)\n\n# fit indexer pipeline on df and transform\nindexed = pipeline.fit(df).transform(df)\nindexed.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:45:19.533848Z","iopub.execute_input":"2022-04-21T10:45:19.534445Z","iopub.status.idle":"2022-04-21T10:45:22.877799Z","shell.execute_reply.started":"2022-04-21T10:45:19.534403Z","shell.execute_reply":"2022-04-21T10:45:22.876868Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"## correlation matrix\nfrom pyspark.ml.stat import Correlation\n\n# convert to vector column first\nvector_col = 'corr_features'\ncorr_cols = list(set(indexed.columns) - set([\"department\", \"employee_name\", \"state\"]))\n\n# vector assembler\nassembler = VectorAssembler(inputCols=corr_cols, outputCol=vector_col)\nindexed_vector = assembler.transform(indexed).select(vector_col)\n\n# get correlation matrix\nmatrix = Correlation.corr(indexed_vector, vector_col)\n\nresult = matrix.collect()[0]['pearson({})'.format(vector_col)].values\nresult","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:50:48.301604Z","iopub.execute_input":"2022-04-21T10:50:48.301932Z","iopub.status.idle":"2022-04-21T10:50:50.376707Z","shell.execute_reply.started":"2022-04-21T10:50:48.301895Z","shell.execute_reply":"2022-04-21T10:50:50.375868Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# corr matrix as a DataFrame\npd.DataFrame(result.reshape(-1, len(corr_cols)), columns=corr_cols, index=corr_cols)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:51:54.992838Z","iopub.execute_input":"2022-04-21T10:51:54.993117Z","iopub.status.idle":"2022-04-21T10:51:55.009314Z","shell.execute_reply.started":"2022-04-21T10:51:54.993090Z","shell.execute_reply":"2022-04-21T10:51:55.008483Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}